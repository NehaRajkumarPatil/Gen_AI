{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4eb58ff3",
      "metadata": {
        "id": "4eb58ff3"
      },
      "source": [
        "# Unit 1 – Model Benchmark Challenge\n",
        "\n",
        "**Name:** Neha Patil  \n",
        "**SRN:** PES2UG23CS379  \n",
        "**SEC:** F\n",
        "\n",
        "## Objective\n",
        "To evaluate how model architecture affects task performance by forcing BERT, RoBERTa, and BART to perform tasks they may not be designed for.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2715c2c4",
      "metadata": {
        "id": "2715c2c4"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e12c81e4",
      "metadata": {
        "id": "e12c81e4"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c1ede17",
      "metadata": {
        "id": "9c1ede17"
      },
      "source": [
        "## Models Used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cb41a6bc",
      "metadata": {
        "id": "cb41a6bc"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce62b766",
      "metadata": {
        "id": "ce62b766"
      },
      "source": [
        "## Experiment 1: Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a593cc40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a593cc40",
        "outputId": "0a106046-6d28-4cad-f3ae-c76b1d9bd5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT (bert-base-uncased)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a revolutionary technology that........................................\n",
            "\n",
            "RoBERTa (roberta-base)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a revolutionary technology that.\n",
            "\n",
            "BART (facebook/bart-base)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a revolutionary technology thatagged249andAcross cry aqu Knightdevdevdevodox tunnel braces braces Favor Bar crykees Salvador Bar249 braces artif cryRNA Jets cry braces braces249Issue Baruranceuranceurance Barurance Bar\u0013\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Generative AI is a revolutionary technology that\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} ({model})\")\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model, truncation=True)\n",
        "        output = generator(prompt, max_new_tokens=40, do_sample=True)\n",
        "        print(output[0][\"generated_text\"])\n",
        "    except Exception as e:\n",
        "        print(\"Generation failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c04c444",
      "metadata": {
        "id": "3c04c444"
      },
      "source": [
        "## Experiment 2: Fill-Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5cc43686",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cc43686",
        "outputId": "a5b767d0-51bc-4ee8-c532-ea81559a7acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT (bert-base-uncased)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create → score: 0.5397\n",
            "generate → score: 0.1558\n",
            "produce → score: 0.0541\n",
            "\n",
            "RoBERTa (roberta-base)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " generate → score: 0.3711\n",
            " create → score: 0.3677\n",
            " discover → score: 0.0835\n",
            "\n",
            "BART (facebook/bart-base)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " create → score: 0.0746\n",
            " help → score: 0.0657\n",
            " provide → score: 0.0609\n"
          ]
        }
      ],
      "source": [
        "sentences = {\n",
        "    \"BERT\": \"The goal of Generative AI is to [MASK] new content.\",\n",
        "    \"RoBERTa\": \"The goal of Generative AI is to <mask> new content.\",\n",
        "    \"BART\": \"The goal of Generative AI is to <mask> new content.\"\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} ({model})\")\n",
        "    try:\n",
        "        fill = pipeline(\"fill-mask\", model=model)\n",
        "        results = fill(sentences[name])\n",
        "        for r in results[:3]:\n",
        "            print(f\"{r['token_str']} → score: {r['score']:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(\"Fill-mask failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf8ce4f3",
      "metadata": {
        "id": "bf8ce4f3"
      },
      "source": [
        "## Experiment 3: Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "95c49f26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95c49f26",
        "outputId": "a0d22540-bacc-4f47-ef9a-42062e8de2ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT (bert-base-uncased)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Generative\n",
            "Score: 0.0090933449100703\n",
            "\n",
            "RoBERTa (roberta-base)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Generative AI poses significant risks such\n",
            "Score: 0.007348828483372927\n",
            "\n",
            "BART (facebook/bart-base)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: such\n",
            "Score: 0.012322017922997475\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} ({model})\")\n",
        "    try:\n",
        "        qa = pipeline(\n",
        "            \"question-answering\",\n",
        "            model=model,\n",
        "            handle_impossible_answer=True\n",
        "        )\n",
        "        result = qa(\n",
        "            question=question,\n",
        "            context=context\n",
        "        )\n",
        "        print(\"Answer:\", result[\"answer\"])\n",
        "        print(\"Score:\", result[\"score\"])\n",
        "    except Exception as e:\n",
        "        print(\"QA failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aab4494",
      "metadata": {
        "id": "7aab4494"
      },
      "source": [
        "## Observation Table\n",
        "\n",
        "| Task | Model | Classification | Observation | Architectural Reason |\n",
        "|------|-------|---------------|-------------|----------------------|\n",
        "| Text Generation | BERT | Failure | Incoherent or repetitive output | Encoder-only model |\n",
        "|  | RoBERTa | Failure | Similar incoherent continuation | Encoder-only model |\n",
        "|  | BART | Success | Coherent text generation | Encoder–Decoder architecture |\n",
        "| Fill-Mask | BERT | Success | Correct predictions | Trained with MLM |\n",
        "|  | RoBERTa | Success | High-quality predictions | Optimized MLM |\n",
        "|  | BART | Partial | Inconsistent results | Not designed for MLM |\n",
        "| QA | BERT | Partial | Keyword-level answers | Not QA fine-tuned |\n",
        "|  | RoBERTa | Partial | Unstable answers | Not QA fine-tuned |\n",
        "|  | BART | Partial | Sometimes coherent | No QA fine-tuning |\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}